"""
SerpAPI Google Search scraper
Uses official SerpAPI to get real Google search results
"""
import asyncio
from datetime import datetime
from typing import List
import httpx
from .base import BaseScraper, Review


class SerpAPIScraper(BaseScraper):
    """Scrape Google Search results using SerpAPI"""
    
    def __init__(self, brand_name: str, api_key: str):
        super().__init__(brand_name)
        self.api_key = api_key
        self.base_url = "https://serpapi.com/search"
        self.timeout = 30
    
    async def scrape(self) -> List[Review]:
        """Scrape Google search results for brand"""
        print(f"\n[SerpAPI] æœå°‹å“ç‰Œ: {self.brand_name}")
        
        reviews = []
        
        # Generate search queries - more comprehensive
        queries = [
            f"{self.brand_name} è©•è«–",
            f"{self.brand_name} ç¼ºé»",
            f"{self.brand_name} å“è³ª",
            f"{self.brand_name} ä¸å¥½",
            f"{self.brand_name} review",
        ]
        
        for query in queries:  # Search all queries
            try:
                print(f"  ğŸ” æŸ¥è©¢: {query}")
                results = await self._search(query)
                reviews.extend(results)
                print(f"    âœ“ æ‰¾åˆ° {len(results)} å€‹çµæœ")
            except Exception as e:
                print(f"    âœ— éŒ¯èª¤: {e}")
        
        # Remove duplicates by URL
        seen_urls = set()
        unique_reviews = []
        for review in reviews:
            if review.url not in seen_urls:
                seen_urls.add(review.url)
                unique_reviews.append(review)
        
        print(f"âœ“ ç¸½å…± {len(unique_reviews)} å€‹ç¨ç‰¹çµæœ\n")
        return unique_reviews
    
    async def scrape_by_url(self, url: str) -> List[Review]:
        """Not used"""
        return []
    
    async def _search(self, query: str) -> List[Review]:
        """Search Google via SerpAPI"""
        reviews = []
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                params = {
                    "q": query,
                    "engine": "google",
                    "api_key": self.api_key,
                    "num": 20,  # Get 20 results instead of 10
                    "hl": "zh-TW",  # Traditional Chinese
                }
                
                response = await client.get(self.base_url, params=params)
                response.raise_for_status()
                
                data = response.json()
                
                # Debug: print raw response
                print(f"      API è¿”å› {len(data.get('organic_results', []))} å€‹çµæœ")
                
                # Extract organic results
                organic_results = data.get("organic_results", [])
                
                # Debug: print first result to see structure
                if organic_results:
                    print(f"      ç¬¬ä¸€ç­†çµæœ: {organic_results[0]}")
                
                for result in organic_results:
                    try:
                        title = result.get("title", "")
                        url = result.get("link", "")
                        snippet = result.get("snippet", "")
                        
                        # Skip if no title or no URL
                        if not title:
                            continue
                        
                        # If URL is missing but we have content, still include it
                        if not url:
                            url = ""
                        
                        review = Review(
                            source="google",
                            title=title,
                            content=snippet[:500] if snippet else "(ç„¡æ‘˜è¦)",
                            author="Google Search",
                            rating=None,
                            url=url,
                            scraped_at=datetime.now(),
                            posted_at=None,
                        )
                        reviews.append(review)
                        
                    except Exception as e:
                        print(f"      è§£æéŒ¯èª¤: {e}")
                        continue
        
        except Exception as e:
            print(f"      SerpAPI éŒ¯èª¤: {e}")
        
        return reviews


# Test
async def test_serpapi():
    api_key = "YOUR_API_KEY"  # Replace with actual key
    scraper = SerpAPIScraper("Apple", api_key)
    
    print("æ¸¬è©¦ SerpAPI çˆ¬èŸ²")
    print("=" * 60)
    
    reviews = await scraper.scrape()
    
    print(f"æ‰¾åˆ° {len(reviews)} ç¯‡çµæœ\n")
    
    for i, review in enumerate(reviews[:5], 1):
        print(f"{i}. {review.title}")
        print(f"   {review.content[:80]}...")
        print(f"   URL: {review.url[:70]}...")
        print()


if __name__ == "__main__":
    asyncio.run(test_serpapi())
